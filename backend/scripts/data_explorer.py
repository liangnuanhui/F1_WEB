#!/usr/bin/env python3
"""
FastF1 æ•°æ®æ¢ç´¢å·¥å…·
ç”¨äºåˆ†æ FastF1 çš„å®é™…æ•°æ®ç»“æ„ï¼Œä¸ºæ•°æ®å»ºæ¨¡æä¾›ä¾æ®
ç»“æœä¿å­˜ä¸º Markdown æ–‡ä»¶
"""

import sys
import os
import logging
import pandas as pd
from datetime import datetime
from io import StringIO
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å®šä¹‰æˆ‘ä»¬éœ€è¦çš„èµ›å­£èŒƒå›´
TARGET_SEASONS = [2023, 2024, 2025]

# ç”¨äºæ•è·è¾“å‡ºçš„ StringIO å¯¹è±¡
output_buffer = StringIO()

def log_and_capture(message, level="INFO"):
    """è®°å½•æ—¥å¿—å¹¶æ•è·è¾“å‡º"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    formatted_message = f"[{timestamp}] {level}: {message}"
    
    # è¾“å‡ºåˆ°æ§åˆ¶å°
    print(formatted_message)
    
    # åŒæ—¶å†™å…¥åˆ° Markdown ç¼“å†²åŒº
    output_buffer.write(formatted_message + "\n")
    
    # è®°å½•åˆ°æ—¥å¿—
    if level == "ERROR":
        logger.error(message)
    elif level == "WARNING":
        logger.warning(message)
    else:
        logger.info(message)

def explore_data_structure(data, name):
    """æ¢ç´¢æ•°æ®ç»“æ„"""
    log_and_capture(f"\n{'='*60}")
    log_and_capture(f"ğŸ“Š {name} æ•°æ®ç»“æ„åˆ†æ")
    log_and_capture(f"{'='*60}")
    
    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯ FastF1 çš„ ErgastMultiResponse ç±»å‹
        if hasattr(data, '__class__') and 'ErgastMultiResponse' in str(data.__class__):
            log_and_capture(f"ğŸ“‹ æ•°æ®ç±»å‹: ErgastMultiResponse")
            
            # æ˜¾ç¤ºæè¿°ä¿¡æ¯
            if hasattr(data, 'description') and not data.description.empty:
                log_and_capture(f"ğŸ“ æè¿°ä¿¡æ¯ (å…±{len(data.description)}ä¸ªæ¯”èµ›):")
                log_and_capture(f"```")
                log_and_capture(data.description.to_string())
                log_and_capture(f"```")
            
            # æ˜¾ç¤ºå†…å®¹æ•°æ®
            if hasattr(data, 'content') and data.content:
                log_and_capture(f"ğŸ“ å†…å®¹æ•°é‡: {len(data.content)} ä¸ªæ•°æ®é›†")
                
                for idx, df in enumerate(data.content):
                    log_and_capture(f"\nğŸ“Š ç¬¬ {idx + 1} ä¸ªæ•°æ®é›†:")
                    log_and_capture(f"   æ•°æ®ç±»å‹: DataFrame")
                    log_and_capture(f"   æ•°æ®å½¢çŠ¶: {df.shape}")
                    log_and_capture(f"   åˆ—å: {list(df.columns)}")
                    
                    # æ˜¾ç¤ºå¯¹åº”çš„æè¿°ä¿¡æ¯
                    if hasattr(data, 'description') and idx < len(data.description):
                        race_info = data.description.iloc[idx]
                        log_and_capture(f"   å¯¹åº”æ¯”èµ›: ç¬¬{race_info.get('round', 'N/A')}è½® - {race_info.get('raceName', 'N/A')}")
                    
                    print(f"\n   æ•°æ®ç±»å‹:")
                    for col, dtype in df.dtypes.items():
                        log_and_capture(f"     {col}: {dtype}")
                    
                    if not df.empty:
                        log_and_capture(f"\n   ç¤ºä¾‹æ•°æ® (å‰3è¡Œ):")
                        log_and_capture(f"```")
                        log_and_capture(df.head(3).to_string())
                        log_and_capture(f"```")
                        
                        log_and_capture(f"\n   æ•°æ®ç»Ÿè®¡:")
                        log_and_capture(f"   éç©ºå€¼ç»Ÿè®¡:")
                        for col in df.columns:
                            try:
                                non_null_count = df[col].notna().sum()
                                null_count = df[col].isna().sum()
                                total_count = len(df)
                                if total_count > 0:
                                    percentage = (non_null_count / total_count) * 100
                                    log_and_capture(f"     {col}: {non_null_count}/{total_count} ({percentage:.1f}%) éç©º, {null_count} ç©ºå€¼")
                                else:
                                    log_and_capture(f"     {col}: 0/0 (0.0%) éç©º, 0 ç©ºå€¼")
                            except Exception as e:
                                log_and_capture(f"     {col}: ç»Ÿè®¡å¤±è´¥ - {e}", "ERROR")
                        
                        # æ£€æŸ¥å”¯ä¸€å€¼
                        log_and_capture(f"\n   å”¯ä¸€å€¼ç»Ÿè®¡:")
                        for col in df.columns:
                            try:
                                unique_count = df[col].nunique()
                                log_and_capture(f"     {col}: {unique_count} ä¸ªå”¯ä¸€å€¼")
                                if unique_count <= 10 and unique_count > 0:
                                    unique_values = df[col].dropna().unique()
                                    # ç¡®ä¿å€¼æ˜¯å¯å“ˆå¸Œçš„
                                    safe_values = []
                                    for val in unique_values:
                                        try:
                                            hash(val)
                                            safe_values.append(str(val))
                                        except:
                                            safe_values.append(f"<ä¸å¯å“ˆå¸Œç±»å‹: {type(val).__name__}>")
                                    log_and_capture(f"       å”¯ä¸€å€¼: {safe_values}")
                            except Exception as e:
                                log_and_capture(f"     {col}: å”¯ä¸€å€¼ç»Ÿè®¡å¤±è´¥ - {e}", "ERROR")
                        
                        # æ£€æŸ¥æ•°å€¼åˆ—çš„ç»Ÿè®¡ä¿¡æ¯
                        try:
                            numeric_cols = df.select_dtypes(include=['number']).columns
                            if len(numeric_cols) > 0:
                                log_and_capture(f"\n   æ•°å€¼åˆ—ç»Ÿè®¡:")
                                log_and_capture(f"```")
                                log_and_capture(df[numeric_cols].describe().to_string())
                                log_and_capture(f"```")
                        except Exception as e:
                            log_and_capture(f"   æ•°å€¼åˆ—ç»Ÿè®¡å¤±è´¥: {e}", "ERROR")
                    else:
                        log_and_capture(f"   æ•°æ®ä¸ºç©º")
            else:
                log_and_capture("âŒ æ²¡æœ‰å†…å®¹æ•°æ®", "ERROR")
        
        elif isinstance(data, pd.DataFrame):
            log_and_capture(f"ğŸ“‹ æ•°æ®ç±»å‹: DataFrame")
            log_and_capture(f"ğŸ“ æ•°æ®å½¢çŠ¶: {data.shape}")
            log_and_capture(f"ğŸ“ åˆ—å: {list(data.columns)}")
            
            log_and_capture(f"\nğŸ“Š æ•°æ®ç±»å‹:")
            for col, dtype in data.dtypes.items():
                log_and_capture(f"   {col}: {dtype}")
            
            if not data.empty:
                log_and_capture(f"\nğŸ“‹ ç¤ºä¾‹æ•°æ® (å‰3è¡Œ):")
                log_and_capture(f"```")
                log_and_capture(data.head(3).to_string())
                log_and_capture(f"```")
                
                log_and_capture(f"\nğŸ” æ•°æ®ç»Ÿè®¡:")
                log_and_capture(f"   éç©ºå€¼ç»Ÿè®¡:")
                for col in data.columns:
                    try:
                        non_null_count = data[col].notna().sum()
                        null_count = data[col].isna().sum()
                        total_count = len(data)
                        if total_count > 0:
                            percentage = (non_null_count / total_count) * 100
                            log_and_capture(f"     {col}: {non_null_count}/{total_count} ({percentage:.1f}%) éç©º, {null_count} ç©ºå€¼")
                        else:
                            log_and_capture(f"     {col}: 0/0 (0.0%) éç©º, 0 ç©ºå€¼")
                    except Exception as e:
                        log_and_capture(f"     {col}: ç»Ÿè®¡å¤±è´¥ - {e}", "ERROR")
                
                # æ£€æŸ¥å”¯ä¸€å€¼
                log_and_capture(f"\nğŸ¯ å”¯ä¸€å€¼ç»Ÿè®¡:")
                for col in data.columns:
                    try:
                        unique_count = data[col].nunique()
                        log_and_capture(f"     {col}: {unique_count} ä¸ªå”¯ä¸€å€¼")
                        if unique_count <= 10 and unique_count > 0:
                            unique_values = data[col].dropna().unique()
                            # ç¡®ä¿å€¼æ˜¯å¯å“ˆå¸Œçš„
                            safe_values = []
                            for val in unique_values:
                                try:
                                    hash(val)
                                    safe_values.append(str(val))
                                except:
                                    safe_values.append(f"<ä¸å¯å“ˆå¸Œç±»å‹: {type(val).__name__}>")
                            log_and_capture(f"       å”¯ä¸€å€¼: {safe_values}")
                    except Exception as e:
                        log_and_capture(f"     {col}: å”¯ä¸€å€¼ç»Ÿè®¡å¤±è´¥ - {e}", "ERROR")
                
                # æ£€æŸ¥æ•°å€¼åˆ—çš„ç»Ÿè®¡ä¿¡æ¯
                try:
                    numeric_cols = data.select_dtypes(include=['number']).columns
                    if len(numeric_cols) > 0:
                        log_and_capture(f"\nğŸ“ˆ æ•°å€¼åˆ—ç»Ÿè®¡:")
                        log_and_capture(f"```")
                        log_and_capture(data[numeric_cols].describe().to_string())
                        log_and_capture(f"```")
                except Exception as e:
                    log_and_capture(f"   æ•°å€¼åˆ—ç»Ÿè®¡å¤±è´¥: {e}", "ERROR")
            else:
                log_and_capture("âŒ æ•°æ®ä¸ºç©º")
        
        else:
            log_and_capture(f"ğŸ“‹ æ•°æ®ç±»å‹: {type(data).__name__}")
            log_and_capture(f"ğŸ“ æ•°æ®é•¿åº¦: {len(data) if hasattr(data, '__len__') else 'N/A'}")
            
            # å°è¯•è½¬æ¢ä¸º DataFrame
            try:
                if hasattr(data, 'to_dataframe'):
                    df = data.to_dataframe()
                    log_and_capture(f"âœ… æˆåŠŸè½¬æ¢ä¸º DataFrame")
                    explore_data_structure(df, f"{name} (è½¬æ¢å)")
                else:
                    log_and_capture(f"âŒ æ— æ³•è½¬æ¢ä¸º DataFrame")
            except Exception as e:
                log_and_capture(f"âŒ è½¬æ¢å¤±è´¥: {e}", "ERROR")
    
    except Exception as e:
        log_and_capture(f"âŒ æ¢ç´¢æ•°æ®ç»“æ„å¤±è´¥: {e}", "ERROR")

def explore_fastf1_data():
    """æ¢ç´¢ FastF1 æ•°æ®"""
    log_and_capture("ğŸš€ å¼€å§‹æ¢ç´¢ FastF1 æ•°æ®ç»“æ„")
    log_and_capture("="*80)
    
    try:
        import fastf1
        
        for season in TARGET_SEASONS:
            log_and_capture(f"\nğŸ† æ¢ç´¢ {season} èµ›å­£æ•°æ®")
            log_and_capture("-"*40)
            
            try:
                # 1. æ¢ç´¢æ¯”èµ›æ—¥ç¨‹
                log_and_capture(f"ğŸ“… è·å– {season} èµ›å­£æ¯”èµ›æ—¥ç¨‹...")
                schedule = fastf1.get_event_schedule(season)
                explore_data_structure(schedule, f"{season}èµ›å­£æ¯”èµ›æ—¥ç¨‹")
                
                # 2. æ¢ç´¢è½¦æ‰‹ä¿¡æ¯
                log_and_capture(f"ğŸ‘¨â€ğŸ è·å– {season} èµ›å­£è½¦æ‰‹ä¿¡æ¯...")
                drivers = fastf1.get_driver_info(season)
                explore_data_structure(drivers, f"{season}èµ›å­£è½¦æ‰‹ä¿¡æ¯")
                
                # 3. æ¢ç´¢è½¦é˜Ÿä¿¡æ¯
                log_and_capture(f"ğŸï¸ è·å– {season} èµ›å­£è½¦é˜Ÿä¿¡æ¯...")
                constructors = fastf1.get_constructor_info(season)
                explore_data_structure(constructors, f"{season}èµ›å­£è½¦é˜Ÿä¿¡æ¯")
                
                # 4. æ¢ç´¢ç§¯åˆ†æ¦œ
                log_and_capture(f"ğŸ† è·å– {season} èµ›å­£ç§¯åˆ†æ¦œ...")
                standings = fastf1.get_driver_standings(season)
                explore_data_structure(standings, f"{season}èµ›å­£ç§¯åˆ†æ¦œ")
                
                # 5. æ¢ç´¢æ¯”èµ›ç»“æœ
                log_and_capture(f"ğŸ è·å– {season} èµ›å­£æ¯”èµ›ç»“æœ...")
                results = fastf1.get_race_results(season)
                explore_data_structure(results, f"{season}èµ›å­£æ¯”èµ›ç»“æœ")
                
                # 6. æ¢ç´¢æ’ä½èµ›ç»“æœ
                log_and_capture(f"â±ï¸ è·å– {season} èµ›å­£æ’ä½èµ›ç»“æœ...")
                qualifying = fastf1.get_qualifying_results(season)
                explore_data_structure(qualifying, f"{season}èµ›å­£æ’ä½èµ›ç»“æœ")
                
                # 7. æ¢ç´¢å†²åˆºèµ›ç»“æœ
                log_and_capture(f"âš¡ è·å– {season} èµ›å­£å†²åˆºèµ›ç»“æœ...")
                sprint = fastf1.get_sprint_results(season)
                explore_data_structure(sprint, f"{season}èµ›å­£å†²åˆºèµ›ç»“æœ")
                
            except Exception as e:
                log_and_capture(f"âŒ {season} èµ›å­£æ•°æ®æ¢ç´¢å¤±è´¥: {e}", "ERROR")
                continue
    
    except ImportError:
        log_and_capture("âŒ FastF1 åº“æœªå®‰è£…", "ERROR")
    except Exception as e:
        log_and_capture(f"âŒ æ¢ç´¢å¤±è´¥: {e}", "ERROR")

def generate_model_suggestions():
    """ç”Ÿæˆæ•°æ®æ¨¡å‹å»ºè®®"""
    log_and_capture("\n" + "="*80)
    log_and_capture("ğŸ—ï¸ æ•°æ®æ¨¡å‹å»ºè®®")
    log_and_capture("="*80)
    
    log_and_capture("""
åŸºäº FastF1 æ•°æ®ç»“æ„åˆ†æï¼Œå»ºè®®çš„æ•°æ®æ¨¡å‹è®¾è®¡ï¼š

## 1. æ ¸å¿ƒå®ä½“

### Season (èµ›å­£)
- id: ä¸»é”®
- year: å¹´ä»½
- name: èµ›å­£åç§°
- start_date: å¼€å§‹æ—¥æœŸ
- end_date: ç»“æŸæ—¥æœŸ
- is_current: æ˜¯å¦å½“å‰èµ›å­£

### Circuit (èµ›é“)
- id: ä¸»é”®
- circuit_name: èµ›é“åç§°
- country: å›½å®¶
- locality: åŸå¸‚
- latitude: çº¬åº¦
- longitude: ç»åº¦

### Constructor (è½¦é˜Ÿ)
- id: ä¸»é”®
- constructor_name: è½¦é˜Ÿåç§°
- constructor_nationality: å›½ç±
- season_id: å…³è”èµ›å­£

### Driver (è½¦æ‰‹)
- id: ä¸»é”®
- driver_number: è½¦æ‰‹å·ç 
- driver_code: è½¦æ‰‹ä»£ç 
- given_name: å
- family_name: å§“
- driver_nationality: å›½ç±
- date_of_birth: å‡ºç”Ÿæ—¥æœŸ

### Race (æ¯”èµ›)
- id: ä¸»é”®
- season_id: å…³è”èµ›å­£
- circuit_id: å…³è”èµ›é“
- round_number: è½®æ¬¡
- official_event_name: å®˜æ–¹æ¯”èµ›åç§°
- event_date: æ¯”èµ›æ—¥æœŸ
- event_format: æ¯”èµ›æ ¼å¼ (conventional, sprint_qualifying)
- is_sprint: æ˜¯å¦å†²åˆºèµ›

## 2. ç»“æœå®ä½“

### Result (æ¯”èµ›ç»“æœ)
- id: ä¸»é”®
- race_id: å…³è”æ¯”èµ›
- driver_id: å…³è”è½¦æ‰‹
- constructor_id: å…³è”è½¦é˜Ÿ
- position: åæ¬¡
- points: ç§¯åˆ†
- grid_position: å‘è½¦ä½ç½®
- status: çŠ¶æ€
- finish_time: å®Œèµ›æ—¶é—´

### QualifyingResult (æ’ä½èµ›ç»“æœ)
- id: ä¸»é”®
- race_id: å…³è”æ¯”èµ›
- driver_id: å…³è”è½¦æ‰‹
- constructor_id: å…³è”è½¦é˜Ÿ
- position: åæ¬¡
- q1_time: Q1æ—¶é—´
- q2_time: Q2æ—¶é—´
- q3_time: Q3æ—¶é—´

### SprintResult (å†²åˆºèµ›ç»“æœ)
- id: ä¸»é”®
- race_id: å…³è”æ¯”èµ›
- driver_id: å…³è”è½¦æ‰‹
- constructor_id: å…³è”è½¦é˜Ÿ
- position: åæ¬¡
- points: ç§¯åˆ†
- grid_position: å‘è½¦ä½ç½®
- status: çŠ¶æ€
- finish_time: å®Œèµ›æ—¶é—´

## 3. å…³ç³»è®¾è®¡

### DriverSeason (è½¦æ‰‹èµ›å­£å…³ç³»)
- id: ä¸»é”®
- driver_id: å…³è”è½¦æ‰‹
- constructor_id: å…³è”è½¦é˜Ÿ
- season_id: å…³è”èµ›å­£
- driver_number: è½¦æ‰‹å·ç 

## 4. å»ºè®®

1. ä½¿ç”¨å¤–é”®çº¦æŸç¡®ä¿æ•°æ®å®Œæ•´æ€§
2. ä¸ºå¸¸ç”¨æŸ¥è¯¢å­—æ®µæ·»åŠ ç´¢å¼•
3. è€ƒè™‘ä½¿ç”¨æšä¸¾ç±»å‹å®šä¹‰æ¯”èµ›æ ¼å¼å’ŒçŠ¶æ€
4. å®ç°è½¯åˆ é™¤æœºåˆ¶ä¿ç•™å†å²æ•°æ®
5. æ·»åŠ åˆ›å»ºæ—¶é—´å’Œæ›´æ–°æ—¶é—´å­—æ®µç”¨äºå®¡è®¡
""")

def main():
    """ä¸»å‡½æ•°"""
    log_and_capture("ğŸ¯ FastF1 æ•°æ®æ¢ç´¢å·¥å…·")
    log_and_capture("="*80)
    
    # æ¢ç´¢æ•°æ®
    explore_fastf1_data()
    
    # ç”Ÿæˆå»ºè®®
    generate_model_suggestions()
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"fastf1_data_exploration_{timestamp}.md"
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# FastF1 æ•°æ®ç»“æ„æ¢ç´¢æŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(output_buffer.getvalue())
        
        log_and_capture(f"âœ… æ¢ç´¢æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        log_and_capture(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}", "ERROR")

if __name__ == "__main__":
    main() 